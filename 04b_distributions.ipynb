{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Machine Learning - Probability distributions\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)\n",
    "\n",
    "In this course we will cover\n",
    "1. A [quick recap](#recap) on simple probability concepts\n",
    "2. An introduction to [probability distributions](#distributions)\n",
    "3. An explanation on how to [sample](#sampling) from distributions ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"recap\"> </a>\n",
    "\n",
    "## Quick recap on probability\n",
    "\n",
    "The field of probability aims to model random or uncertain events through random variables $X$ denoting a quantity that is uncertain, which take values $\\omega$ in a sample space $\\omega \\in \\Omega$. If we observe several occurrences of this variable $\\{\\mathbf{x}_{i}\\}_{i=1}$, we might try to model the _probability distribution_ $p(\\mathbf{x})$ of that variable. We recall here that the probability of an event $a$ is a real number $p(a)$, with $0 \\leq p(a) \\leq 1$, knowing that $p(\\Omega)=1$ and $p(\\{\\})=0$. The probability of two events occuring simultaneously is defined as $p\\left(a \\cup b \\right)$. Therefore, the probability of one event **or** the other occuring is defined as\n",
    "$\n",
    "\\begin{equation}\n",
    "p\\left(a \\cap b \\right) = p(a) + p(b) - p\\left(a \\cup b \\right)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "The *conditional probability* of an event $a$ occuring *given* another event $b$ is denoted $p \\left(a \\mid b \\right)$ and is defined as\n",
    "$$\n",
    "\\begin{equation}\n",
    "p \\left(a \\mid b \\right) = \\frac{p \\left(a , b \\right)}{p \\left(b \\right)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This can be understood as the probability of event $a$ to occur if we restrict the world of possibilities to event $b$. The *chain rule* defines the probabilities of a set of events to co-occur simultaneously\n",
    "$$\n",
    "\\begin{equation}\n",
    "p \\left(x_{1},...,x_{n} \\right) = \\prod_{i=n}^{1}{p \\left(x_{i}\\mid x_{i-1},..., x_{1} \\right)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Finally, we say that two events are independent if $p(a\\mid b) = p(a)$.\n",
    "\n",
    "To understand these concepts graphically, we will rely on `PyTorch` and specifically the `distributions` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as distribution\n",
    "import torch.distributions.transforms as transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from helper_plot import hdr_plot_style\n",
    "hdr_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distributions\"> </a>\n",
    "### Probability distributions\n",
    "\n",
    "Let $X$ be a random variable associated with a *probability distribution function* that assigns probabilities to the different outcomes $X$ can take in the sample space. We can divide random variables into three different types:\n",
    "\n",
    "- **$X$ is discrete**: Discrete random variables may only assume values on a specified list. \n",
    "- **$X$ is continuous**: Continuous random variable can take on arbitrarily exact values. \n",
    "- **$X$ is mixed**: Mixed random variables assign probabilities to both discrete and continuous random variables, (i.e. a combination of the above two categories). \n",
    "\n",
    "**Expected Value**\n",
    "The expected value $\\mathbb{E}\\left[X\\right]$ for a given probability distribution can be described as _\"the mean expected value for many repeated samples from that distribution.\"_ As the number of repeated observation goes to infinity, the difference between the average outcome and the expected value becomes arbitrarily small. Formally, it is defined for discrete variables as\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[X\\right] = \\sum\\limits_{i}\\bx_i p(\\bx=\\bx_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete distributions\n",
    "\n",
    "If $X$ is discrete, then its distribution is called a *probability mass function* (pmf), which measures the probability $X$ takes on the value $x_{i}$, denoted $P(X=x_{i})$. Let $\\mathbf{x}$ be a discrete random variable with range $R_{X}=\\{x_1,\\cdots,x_n\\}$ (finite or countably infinite). The function\n",
    "\n",
    "$$\n",
    "p_{X}(x_{i})=p(X=x_{i}), \\forall i\\in\\{1,\\cdots,n\\}\n",
    "$$\n",
    "\n",
    "is called the probability mass function (PMF) of $X$.\n",
    "\n",
    "Hence, the PMF defines the probabilities of all possible values for a random variable. The above notation allows to express that the PMF is defined for the random variable $X$, so that $p_{X}(1)$ gives the probability that $X=1$. For discrete random variables, the PMF is also called the \\textit{probability distribution}. The PMF is a probability measure, therefore it satisfies all the corresponding properties\n",
    "- $0 \\leq p_{X}(x_i) < 1, \\forall x_i$\n",
    "- $\\sum_{x_i\\in R_{X}} p_{X}(x_i) = 1$\n",
    "- $\\forall A \\subset R_{X}, p(X \\in A)=\\sum_{x_a \\in A}p_{X}(x_a)$\n",
    "\n",
    "A very simple example of discrete distribution is the `Bernoulli` distribution. With this distribution, _we can model a coin flip_, if it has equal probability. More formally, a Bernoulli distribution is defined as\n",
    "\n",
    "$$ \n",
    "Bernoulli(x)= p^x (1-p)^{(1-x)} \n",
    "$$\n",
    "\n",
    "with $p$ controlling the probability of the two classes. Hence, a fair coin should have $p=0.5$, and if we throw the coin a very large number of times, we hope to see on average an equal amount of _heads_ and _tails_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = distribution.Bernoulli(0.5)\n",
    "samples = bernoulli.sample((10000,))\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.distplot(samples)\n",
    "plt.title(\"Samples from a Bernoulli (coin toss)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also _sample_ from the distribution to have individual values of a single throw. In that case, we obtain a series of separate events that _follow_ the distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ['heads', 'tails']\n",
    "samples = bernoulli.sample((10, ))\n",
    "for s in samples:\n",
    "    print('Coin is tossed on ' + vals[int(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can mess up our probability to model an unfair (loaded) coin, as shown in the following example (where we use a cheated coin that should give us a lot more of _heads_ than _tails_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = distribution.Bernoulli(0.3)\n",
    "samples = bernoulli.sample((10000,))\n",
    "plt.figure(figsize=(10, 8)); sns.distplot(samples)\n",
    "plt.title(\"Samples from an unfair coin\"); plt.show()\n",
    "vals = ['heads', 'tails']\n",
    "samples = bernoulli.sample((10, ))\n",
    "for s in samples:\n",
    "    print('Coin is tossed on ' + vals[int(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson distribution\n",
    "\n",
    "Let's introduce one of the (many) useful probability mass functions. We say $Z$ is *Poisson*-distributed if:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P\\left(Z = k\\right) =\\frac{ \\lambda^k e^{-\\lambda} }{k!}, \\; \\; k \\in \\mathbb{N^{+}} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\lambda \\in \\mathbb{R}$ is a parameter of the distribution that controls its shape (usually termed the *intensity* of the Poisson distribution). By increasing $\\lambda$, we add more probability to larger values. One can describe $\\lambda$ as the *intensity* of the Poisson distribution. If a random variable $Z$ has a Poisson mass distribution, we denote it by\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Z \\sim \\text{Poi}(\\lambda) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "One useful property of the Poisson distribution is that its expected value is equal to its parameter, i.e.:\n",
    "\n",
    "$$E\\large[ \\;Z\\; | \\; \\lambda \\;\\large] = \\lambda $$\n",
    "\n",
    "We will plot the probability mass distribution for different $\\lambda$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous distributions\n",
    "\n",
    "The same ideas apply to _continuous_ random variables, which can model for instance the height of human beings. If we try to guess the height of someone that we do not know, there is a higher probability that this person will be around 1m70, instead of 20cm or 3m. For the rest of this course, we will use the shorthand notation $p(\\mathbf{x})$ for the distribution $p(\\mathbf{x}=x_{i})$,  which expresses for a real-valued random variable $\\mathbf{x}$, evaluated at $x_{i}$, the probability that $\\mathbf{x}$ takes the value $x_i$.\n",
    "\n",
    "One notorious example of such distributions is the Gaussian (or Normal) distribution, which is defined as \n",
    "\\begin{equation}\n",
    "    p(x)=\\mathcal{N}(\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "Similarly as before, we can observe the behavior of this distribution with the following code (in our height example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = distribution.Normal(150., 30.)\n",
    "samples = normal.sample((10000, ))\n",
    "sns.distplot(samples)\n",
    "plt.title(\"Samples from a standard Normal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have access to this complete probability distribution (its exact parameterization and function), we can generate samples (in this case \"new humans\") that follow the correct distribution. You can experiment with this in the following code space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of continuous random variable is a random variable with *exponential density*\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f_Z(z | \\lambda) = \\lambda e^{-\\lambda z }, \\;\\; z\\ge 0\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "When a random variable $Z$ has an exponential distribution with parameter $\\lambda$, we say *$Z$ is exponential*\n",
    "\n",
    "$$Z \\sim \\text{Exp}(\\lambda)$$\n",
    "\n",
    "Given a specific $\\lambda$, the expected value of an exponential random variable is equal to the inverse of $\\lambda$, that is\n",
    "\n",
    "$$E[\\; Z \\;|\\; \\lambda \\;] = \\frac{1}{\\lambda}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distribs\"></a>\n",
    "### PyTorch distributions\n",
    "\n",
    "Here, we rely on the [PyTorch distributions module](https://pytorch.org/docs/stable/_modules/torch/distributions/), which is defined in `torch.distributions`. Most notably, we are going to rely both on the `Distribution` and `Transform` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Define grids of points (for later plots)\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "z = np.array(np.meshgrid(x, x)).transpose(1, 2, 0)\n",
    "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this toolbox, we can already find some of the major probability distributions that we are used to deal with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = distrib.Normal(loc=0, scale=1)\n",
    "p = distrib.Bernoulli(probs=torch.tensor([0.5]))\n",
    "p = distrib.Beta(concentration1=torch.tensor([0.5]), concentration0=torch.tensor([0.5]))\n",
    "p = distrib.Gamma(concentration=torch.tensor([1.0]), rate=torch.tensor([1.0]))\n",
    "p = distrib.Pareto(alpha=torch.tensor([1.0]), scale=torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting aspect of these `Distribution` objects is that we can both obtain some samples from it through the `sample` (or `sample_n`) function, but we can also obtain the analytical density at any given point through the `log_prob` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on a normal\n",
    "n = distrib.Normal(0, 1)\n",
    "# Obtain some samples\n",
    "samples = n.sample((1000, ))\n",
    "# Evaluate true density at given points\n",
    "density = torch.exp(n.log_prob(torch.Tensor(x))).numpy()\n",
    "# Plot both samples and density\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15, 4))\n",
    "ax1.hist(samples, 50, alpha=0.8);\n",
    "ax1.set_title('Empirical samples', fontsize=18);\n",
    "ax2.plot(x, density); ax2.fill_between(x, density, 0, alpha=0.5)\n",
    "ax2.set_title('True density', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can experiment with different distributions, and try to compare how they behave depending on their parameters and also on how much _samples_ you draw from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sampling\"></a>\n",
    "## Sampling from distributions\n",
    "\n",
    "The advantage of using probability distributions is that we can *sample* from these to obtain examples that follow the distribution. For instance, if we perform sampling repeatedly (up to infinity) from a Gaussian PDF, the different values will be distributed following the exact Gaussian distribution. However, although we know the PDF, we need to compute the *Cumulative Distribution Function* (CDF), and then its inverse to obtain the sampling function. Therefore, if we denote the PDF as $f_{X}(x)$, we need to compute the CDF \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{X}\\left(x\\right)=\\intop_{\\infty}^{x}f_{X}\\left(t\\right)dt\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Then, the *inverse sampling method* consists in solving and applying the inverse CDF $F_{X}^{-1}\\left(x\\right)$. Here, we recall that the Exponential probability is defined with the following function.\n",
    "\n",
    "$$\n",
    "p_{\\lambda}(x) = \\lambda e^{-\\lambda x}\n",
    "$$\n",
    "\n",
    "with $\\lambda$ defining the _rate_ parameter. Therefore, to be able to define our own `sample` method, we need to solve for\n",
    "\n",
    "$$\n",
    "F^{-1}_Y(x) = \\left(\\int_0^x \\lambda e^{-\\lambda y} dy\\right)^{-1}\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "1. Code the exponential probability density functions \n",
    "2. Perform the inverse transform method on the exponential distribution PDF\n",
    "3. Code the `sample_exponential` function to sample from the exponential\n",
    "4. (optional) Perform the same operations for sampling from the Beta distribution\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "nb_samples = 500\n",
    "nb_bins = 50\n",
    "\n",
    "def sample_exponential(mu, n, m):\n",
    "    \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    \n",
    "    ######################\n",
    "    # Solution\n",
    "\n",
    "    samples = np.random.uniform(0, 1, size=(m,n))\n",
    "    samples = - np.log(1 - samples) \n",
    "    samples *= mu\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Exponential distribution\n",
    "mu = 2\n",
    "samples = np.random.exponential(mu, nb_samples)\n",
    "samples_ex = sample_exponential(mu, nb_samples, 1)\n",
    "# Compute the PDF\n",
    "X = np.linspace(0, np.max(samples), int(np.max(samples)) * 100)\n",
    "y1 = expon.pdf(X) #* (nb_samples / nb_bins) * int(np.max(samples) * 1.5)\n",
    "# Display both\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(samples, 50, label='Samples')\n",
    "plt.plot(X,y1,ls='--',c='r',linewidth=2, label='Exponential PDF')\n",
    "plt.legend(loc=1)\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(samples_ex, 50, label='Samples')\n",
    "plt.plot(X,y1,ls='--',c='r',linewidth=2, label='Our exponential sampples')\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleBeta(a, b, M, N):\n",
    "    \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    \n",
    "    ######################\n",
    "    # Solution\n",
    "\n",
    "    samples = np.random.beta(a, b, size=(M, N))\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "    return samples\n",
    "\n",
    "from scipy.stats import beta\n",
    "# Beta distribution\n",
    "a = 0.6\n",
    "b = 1.5\n",
    "samples = np.random.beta(a, b, nbSamples)\n",
    "samplesBeta = sampleBeta(a, b, nbSamples, 1)\n",
    "# Compute the PDF\n",
    "X = np.linspace(0, 1, 100)\n",
    "y1 = beta.pdf(X, a, b) * (nbSamples / nbBins) * (np.max(samples) * 1.5)\n",
    "# Display both\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(samples, 50, label='Samples')\n",
    "plt.plot(X,y1,ls='--',c='r',linewidth=2, label='Beta PDF')\n",
    "plt.legend(loc=1)\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(samplesBeta, 50, label='Samples')\n",
    "plt.plot(X,y1,ls='--',c='r',linewidth=2, label='Beta PDF')\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing distributions (KL divergence)\n",
    "$\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\bb}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\bx}{\\bb{x}}\n",
    "\\newcommand{\\by}{\\bb{y}}\n",
    "\\newcommand{\\bz}{\\bb{z}}\n",
    "\\newcommand{\\KL}[2]{\\mathcal{D}_{\\text{KL}}\\left[#1 \\| #2\\right]}$\n",
    "Originally defined in the field of information theory, the _Kullback-Leibler (KL) divergence_ (usually noted $\\KL{p(\\bx)}{q(\\bx)}$) is a dissimilarity measure between two probability distributions $p(\\bx)$ and $q(\\bx)$. In the view of information theory, it can be understood as the cost in number of bits necessary for coding samples from $p(\\bx)$ by using a code optimized for $q(\\bx)$ rather than the code optimized for $p(\\bx)$. In the view of probability theory, it represents the amount of information lost when we use $q(\\bx)$ to approximate the true distribution $p(\\bx)$. %that explicit the cost incurred if events were generated by $p(\\bx)$ but charged under $q(\\bx)$\n",
    "\n",
    "\n",
    "Given two probability distributions $p(\\bx)$ and $q(\\bx)$, the Kullback-Leibler divergence of $q(\\bx)$ _from_ $p(\\bx)$ is defined to be\n",
    "\\begin{equation}\n",
    "    \\KL{p(\\bx)}{q(\\bx)}=\\int_{\\R} p(\\bx) \\log \\frac{p(\\bx)}{q(\\bx)}d\\bx\n",
    "\\end{equation}\n",
    "\n",
    "Note that this dissimilarity measure is _asymmetric_, therefore, we have\n",
    "\\begin{equation}\n",
    "    \\KL{p(\\bx)}{q(\\bx)}\\neq \\KL{q(\\bx)}{p(\\bx)}\n",
    "\\end{equation}\n",
    "This asymmetry also describes an interesting behavior of the KL divergence, depending on the order to which it is evaluated. The KL divergence can either be a _mode-seeking_ or _mode-coverage_ measure (we will come back to these notions in the _approximate inference_ course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
