{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music machine learning - Neural networks\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)\n",
    "\n",
    "In this course we will cover\n",
    "1. A [quick introduction](#intro) on the code that we will adress and auxiliary functions\n",
    "2. A simple implementation for a [single neuron](#single)\n",
    "3. An implementation of [regression](#implem) using scikit-learn\n",
    "4. Some [common good practices](#practices) in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will cover a more advanced classification algorithm through the use of *neural networks*. The tutorial starts by performing a simple **single neuron** discrimination of two random distributions. Then, we will study the typical **XOR problem** by using a more advanced 2-layer **perceptron**. Finally, we generalize the use of neural networks in order to perform classification on a given set of audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use relatively _low-level_ libraries to perform the first exercises (implementing your own neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify your work, we provide a first set of functions that provides simple plotting functionnalities (from the `helper_plot.py` file)\n",
    "\n",
    "  |**File**|*Explanation*|\n",
    "  |-------:|:---------|\n",
    "  |`plot_boundary`|Plots the decision boundary of a single neuron with 2-dimensional inputs|\n",
    "  |`plot_patterns`|Plots (bi-dimensionnal) input patterns|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_plot import plot_boundary, plot_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a single neuron is only able to learn _linearly separable_ problems. To produce such classes of problems, we provide a script that draw a set of random 2-dimensional points, then choose a random line in this space that will act as the linear frontier between 2 classes (hence defining a linear 2-class problem). The variables that will be used by your code are the following.  \n",
    "\n",
    "```Matlab\n",
    "desired       % classes of the patterns \n",
    "inputs        % 2 x n final matrix of random input patterns\n",
    "weights       % 2 x 1 vector of neuron weights\n",
    "bias          % 1 x 1 vector of bias\n",
    "```\n",
    "\n",
    "You can execute the code below to see our simple classification problem. (Note that running the same cell multiple times produces a different starting dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points to generate\n",
    "nPats = 30 + np.floor(np.random.rand() * 30);\n",
    "# Generate 2-dimensional random points\n",
    "patterns = np.random.rand(2, int(nPats)) * 2 - 1;\n",
    "# Slope of separating line\n",
    "slope = np.log(np.random.rand() * 10);\n",
    "yint = np.random.rand() * 2 - 1;\n",
    "# Create the indexes for a two-class problem\n",
    "desired = (patterns[1,:] - patterns[0,:] * slope - yint > 0) * 1;\n",
    "# Plot the corresponding pattern\n",
    "fig = plot_patterns(patterns,desired);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single neuron\n",
    "\n",
    "For the first parts of the tutorial, we will perform the simplest classification model possible in a neural network setting, a single neuron. We briefly recall here that; given an input vector $ \\mathbf{x} \\in \\mathbb{R}^{n} $, a single neuron computes the function  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y=\\sigma\\left(\\sum_{i = 1}^{n}w_{i}.x_{i} + b\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with $ \\mathbf{w} \\in \\mathbb{R}^{n} $ a weight vector, $ b $ a bias and $ \\sigma\\left(\\right) $ an *activation function*. Therefore, if we consider the *threshold* activation function ($ \\sigma_0\\left(x\\right)=1 $ if $ x \\geq 0$), a single neuron simply performs an *affine transform* and then a *linear* discrimination of the space. A network will be composed of _layers_ of these neurons, which produce successive computations\n",
    "\n",
    "<img src=\"images/02_feedforward_nn.png\" align=\"center\"/>\n",
    "\n",
    "Geometrically, a single neuron computes an hyperplane that separates the space. In order to learn, we have to adjust the weights and know \"how much wrong we are\". To do so, we consider that we know the desired output $ d $ of a system for a given example $ \\mathbf{x} $ (eg. a predicted value for a regression system, a class value for a classification system). Therefore, we define the loss function $ \\mathcal{L}_{\\mathcal{D}} $ over a whole dataset as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}=\\sum_{j=1}^{k_{\\mathcal{D}}}\\left\\Vert d_{j}-y_{j}\\right\\Vert ^{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In order to know how to change the weights based on the value of the errors, we need to now \"how to change it to make it better\". Therefore, we should compute the sets of derivatives of the error given each parameter\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Delta\\bar{\\mathbf{w}}=\\left(\\frac{\\delta\\mathcal{L}_{\\mathcal{D}}}{\\delta w_{1}},\\ldots,\\frac{\\delta\\mathcal{L}_{\\mathcal{D}}}{\\delta w_{n}}\\right)\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**  \n",
    "\n",
    "  1. Perform the derivatives of the output given a single neuron\n",
    "  2. Perform the derivatives for the bias as well\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each $(x, d)$ input, and output $y$:\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta y}= 2 (y - d)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta y}{\\delta w_k} = x_k \\sigma '\\left(\\sum_{i=1}^{n} w_i x_i + b \\right), \\forall k \\in [|1, n|]\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta w_k}= 2 (y - d) x_k \\sigma '\\left(\\sum_{i=1}^{n} w_i x_i + b \\right)\n",
    "$$\n",
    "Et\n",
    "$$\n",
    "\\frac{\\delta y}{\\delta b} = \\sigma '\\left(\\sum_{i=1}^{n} w_i x_i + b \\right)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta b}= 2 (y - d) \\sigma '\\left(\\sum_{i=1}^{n} w_i x_i + b \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your own neuron\n",
    "\n",
    "We will start by training a single neuron to learn how to perform this discrimination with a linear problem (so that a single neuron is enough to solve it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to use\n",
    "xs = patterns\n",
    "# ground truth\n",
    "ys_truth = desired\n",
    "# Initialize the weights\n",
    "weights = np.random.randn(1, 2)\n",
    "bias = np.random.randn(1, 1)\n",
    "# Learning rate\n",
    "eta = 0.05\n",
    "# Weight decay\n",
    "lambda_w = 0.1\n",
    "\n",
    "print(weights)\n",
    "print(bias)\n",
    "print(xs)\n",
    "\n",
    "fc_layer = lambda w, b:  lambda xs : np.dot(w, xs) + b\n",
    "sigma = lambda xs: 1 / (1 + np.exp(-xs))\n",
    "sigma_d = lambda xs: sigma(xs) * (1 - sigma(xs)) # unoptimised, whatever.\n",
    "loss = lambda ys_out, ys_truth: np.linalg.norm(ys_out - ys_truth)\n",
    "loss_dy =lambda ys_out, ys_truth: 2 * (ys_out - ys_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to update the following code loop to ensure that your neuron learns to separate between the classes \n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "  1. Update the loop so that it computes the forward propagation error\n",
    "  2. Update the loop to perform learning (based on back-propagation)\n",
    "  3. Run the learning procedure, which should produce a result similar to that displayed on the website\n",
    "  4. Perform multiple re-runs by **tweaking the hyperparameters** (learning rate, weight decay)\n",
    "  5. What observations can you make on the learning process?\n",
    "  6. (Optional) Change the input patterns, and confirm your observations.\n",
    "  6. (Optional) Incorporate the bias in the weights to obtain a **vectorized** code.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the corresponding pattern\n",
    "fig = plot_patterns(patterns,desired)\n",
    "plt.draw()\n",
    "# Update loop\n",
    "for i in range(50):\n",
    "    \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    layer = fc_layer(weights, bias)\n",
    "\n",
    "    ls_out = layer(xs)\n",
    "    ys_out = sigma(ls_out)\n",
    "    err = loss(ys_out, ys_truth)\n",
    "\n",
    "    grad_err_b = - loss_dy(ys_out, ys_truth) * sigma_d(ls_out)\n",
    "    grad_err_w = xs * grad_err_b\n",
    "    # We have the gradient for each (x, y) in (xs, ys)\n",
    "    # We can average the result to get the gradient. \n",
    "    grad_err_w_avg = np.mean(grad_err_w, axis=1)\n",
    "    grad_err_b_avg = np.mean(grad_err_b, axis=1)\n",
    "    # update the weights:\n",
    "    weights -= eta * grad_err_w_avg\n",
    "    bias -= eta * grad_err_b_avg\n",
    " \n",
    "    print('%2d.  weights = %f, %f, %f' % (i, bias[0, 0], weights[0, 0], weights[0, 1]))\n",
    "    plot_boundary(np.concatenate((bias, weights), axis=1), i, '--', fig)\n",
    "    plt.draw()\n",
    "    time.sleep(0.2)\n",
    "plot_boundary(np.concatenate((bias, weights), axis=1), i, '-', fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer XOR problem\n",
    "\n",
    "In most cases, classification problems are far from being linear. Therefore, we need more advanced methods to be able to compute non-linear class boundaries. The advantage of neural networks is that the same principle can be applied in a *layer-wise* fashion. This allows to further discriminate the space in sub-regions (as seen in the course). We will try to implement the 2-layer *perceptron* that can provide a solution to the infamous XOR problem. The idea is now to have the output of the first neurons to be connected to a set of other neurons. Therefore, if we take back our previous formulation, we have the same output for the first neuron(s) $y$, that we will now term as $y_{1}$. Then, we feed these outputs to a second layer of neurons, which gives\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_{2}=\\sigma\\left(\\sum_{i = 1}^{n}w_{2}^{i}.y_{1}^{i} + b_{2}\\right)\n",
    "\\end{equation}\n",
    "$$  \n",
    "\n",
    "Finally, we will rely on the same loss $\\mathcal{L_{D}}$ as in the previous exercise, but the outputs used are $y_2$ instead of $y$. As in the previous case, we now need to compute the derivatives of the weights and biases for several layers . However, you should see that some form of generalization might be possible for any number of layer.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Exercise**  \n",
    "  1. Perform the derivatives for the last layer specifically\n",
    "  2. Define a generalized derivative for any previous layer\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For each $(x, d)$ input, and output $y_2$:\n",
    "$$\n",
    "    o_j = \\sum_{i=1}^{n} w_j^i y_j^i + b_j, \\forall j \\in [|1, 2|]\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta y_2}= 2 (y_2 - d)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta y_2}{\\delta o_2} = \\sigma '(o_2)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta o_2}{\\delta y_1} = w_2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta y_1}{\\delta o_1} = \\sigma '(o_1) \n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta o_1}{\\delta w_1} = x\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta y_2}{\\delta w_2} = y_1 \\sigma '(o_2)\n",
    "$$\n",
    "D'où :\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta w_1}= 2 (y_2 - d) x w_2 \\sigma '(o_1) \\sigma '(o_2)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta w_2}= 2 (y_2 - d) y_1 \\sigma '(o_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the prototypical set of XOR values by using the following code (note that this is the most simple case, but still this is typically a problem that cannot be solved by a _linear classifier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = np.array([[-1, -1],[-1,  1],[1, -1],[1,  1]]).transpose() # Input patterns\n",
    "desired = np.array([0, 1, 1, 0])                       # Corresponding classes\n",
    "# Initialize based on their sizes\n",
    "nInputs = patterns.shape[0]\n",
    "nOutputs = 1\n",
    "nPat = patterns.shape[1]\n",
    "# First plot the patterns\n",
    "fig = plot_patterns(patterns, desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables that will be used by your code are the following.\n",
    "\n",
    "```Matlab\n",
    "patterns          % 2 x n matrix of random points\n",
    "desired           % classes of the patterns \n",
    "inputs1           % 3 x n final matrix of inputs (accounting for bias)\n",
    "nHiddens          % Number of hidden units\n",
    "learnRate         % Learning rate parameter\n",
    "momentum          % Momentum parameter (bonus)\n",
    "weights1          % 1st layer weights\n",
    "weights2          % 2nd layer weights\n",
    "TSS_Limit         % Sum-squared error limit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHiddens = 2           # Number of hidden units\n",
    "# Overall input patterns\n",
    "xs = patterns\n",
    "ys_truth = desired\n",
    "tss_limit = 0.02       # Sum-squared error limit\n",
    "# Learning rate\n",
    "eta = 0.01\n",
    "\n",
    "momentum = 0.1\n",
    "lambda_w = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_decay = lambda lambda_w, w: np.linalg.norm(lambda_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Exercise**  \n",
    "\n",
    "  1. Update the forward propagation and error computation (compared to desired).\n",
    "  2. Update the back-propagation part to learn the weights of both layers.\n",
    "  3. Run the learning, which should produce a result similar to that displayed below.\n",
    "  4. Perform multiple re-runs of the learning procedure (re-launching with different initializations)\n",
    "  5. What observations can you make on the learning process?\n",
    "  6. What happens if you initialize all weights to zeros?\n",
    "  7. (Optional) Implement the *sparsity* constraint in your neural network.\n",
    "  7. (Optional) Implement the *weight decay* constraint in your network.\n",
    "  7. (Optional) Add the *momentum* to the learning procedure.\n",
    "  \n",
    "*For optional questions, please look after the first code box for more information*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot the patterns\n",
    "fig = plot_patterns(patterns, desired);\n",
    "\n",
    "\n",
    "# Weights of first and second layer\n",
    "weights1 = (np.random.randn(nHiddens, nInputs) - 0.5)      # 1st layer weights\n",
    "weights2 = (np.random.randn(nOutputs, nHiddens) - 0.5)     # 2nd layer weights\n",
    "bias1 = (np.random.randn(nHiddens, 1) - 0.5)               # 1st layer biases\n",
    "bias2 = (np.random.randn(nOutputs, 1) - 0.5)               # 2nd layer biases\n",
    "# weights1 = np.zeros((nHiddens, nInputs))\n",
    "# weights2 = np.zeros((nOutputs, nHiddens))\n",
    "# bias1 = np.zeros((nHiddens, 1))\n",
    "# bias2 = np.zeros((nOutputs, 1))\n",
    "\n",
    "grad_err_w1 = 0\n",
    "grad_err_w2 = 0\n",
    "grad_err_b1 = 0\n",
    "grad_err_b2 = 0\n",
    "# Iterate for a fixed number of iterations\n",
    "for epoch in range(200):\n",
    "  \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    l1 = fc_layer(weights1, bias1)\n",
    "    l2 = fc_layer(weights2, bias2)\n",
    "\n",
    "    o1 = l1(xs)\n",
    "    y1 = sigma(o1)\n",
    "    o2 = l2(y1)\n",
    "    y2 = sigma(o2)\n",
    "\n",
    "    err = loss(y2, ys_truth) + loss_decay(lambda_w, weights2)\n",
    "    tss = np.linalg.norm(err)\n",
    "\n",
    "    ## Back-propagation\n",
    "    delta2 = sigma_d(o2) * (ys_truth - y2)\n",
    "    delta1 = sigma_d(o1) * np.dot(weights2.transpose(), delta2)\n",
    "\n",
    "    # momentum\n",
    "    weights1 += momentum * grad_err_w1\n",
    "    weights2 += momentum * grad_err_w2\n",
    "    bias1 += momentum * grad_err_b1\n",
    "    bias2 += momentum * grad_err_b2\n",
    "\n",
    "    grad_err_w1 = np.mean(delta1 * xs, axis=1)\n",
    "    grad_err_w2 = np.mean(delta2 * y1, axis=1)\n",
    "    grad_err_b1 = np.mean(delta1, axis=1, keepdims=True)\n",
    "    grad_err_b2 = np.mean(delta2, axis=1, keepdims=True)\n",
    "\n",
    "    # print(grad_err_w1)\n",
    "\n",
    "    weights1 -= eta * grad_err_w1\n",
    "    weights2 -= eta * grad_err_w2\n",
    "    bias1 -= eta * grad_err_b1\n",
    "    bias2 -= eta * grad_err_b2\n",
    "    \n",
    "    # print('Epoch %3d:  Error = %f' % (epoch, tss));\n",
    "    if tss < tss_limit:\n",
    "        break\n",
    "    if (epoch - 1 % 20)==0:\n",
    "        plot_boundary(np.concatenate((bias2, weights2), axis=1), epoch, '--', fig)\n",
    "\n",
    "plot_boundary(np.concatenate((bias1, weights1), axis=1), epoch, '-', fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional questions**\n",
    "\n",
    "2. *Weight decay* constraint\n",
    "\n",
    "As nothing constrains the weights in the network, we can note that usually all weights vector given a multiplicative factor might be equivalent, which can stall the learning (and lead to exploding weights). The *weight decay* allows to regularize the learning by penalizing weights with a too wide amplitude. The idea is to add this constraint as a term to the final loss (which leads to an indirect \"pressure\" on the learning process. Therefore, the final loss will be defined as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{final}=\\mathcal{L_D} + \\lambda \\sum_{l} \\sum_{i} \\sum_{j} \\left( W_{ij}^{l} \\right)^{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the parameter $\\lambda$ controls the relative importance of the two terms.\n",
    "\n",
    "3. *Momentum* in learning\n",
    "\n",
    "Usually, in complex problems, the gradient can be very noisy and, therefore, the learning might oscillate widely. In order to reduce this problem, we can *smooth* the different gradient updates by retaining the values of the gradient at each iteration and then performing an update based on the latest gradient $\\delta_{i}^{t}$ and the gradient at the previous iteration $\\delta_{i}^{t-1}$. Therefore, a gradient update is applied as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_{final}^{t} = \\delta_{i}^{t} + m.\\delta_{i}^{t-1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with $m$ the momentum parameter, which control the amount of gradient smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer audio classification\n",
    "\n",
    "Finally, we will attack a complete audio classification problem and try to perform neural network learning on a set of audio files. The data structure will be the same as the one used for parts 1 and 2. As discussed during the courses, even though a 2-layer neural network can provide non-linear boundaries, it can not perform \"holes\" inside those regions. In order to obtain an improved classification, we will now rely on a 3-layer neural network. The modification to the code of section 3.2 should be minimal, as the back-propagation will be similar for the new layer as one of the two others. We do not develop the math here as it is simply a re-application of the previous rules with an additional layer (which derivatives you should have generalized in the previous exercise).  \n",
    "\n",
    "However, up until now, we only performed *binary classification* problems, but this time we need to obtain a decision rule for multiple classes. Therefore, we cannot rely on simply computing the distance between desired patterns and the obtained binary value. The idea here is to rely on the *softmax regression*, by considering classes as a vector of probabilities. The desired answers will therefore be considered as a set of *probabilities*, where the desired class is $1$ and the others are $0$ (called *one-hot* representation). Then, the cost function will rely on the softmax formulation\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L_D}(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{k} 1\\left\\{y^{(i)} = j\\right\\} log \\frac{e^{\\theta_{j}^{T} x^{(i)}}}{\\sum_{l=1}^{k} e^{ \\theta_{l}^{T} x^{(i)} }}  \\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Therefore, we compute the output of the softmax by taking \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(y^{(i)} = j | x^{(i)}; \\theta) = \\frac{e^{\\theta_{j}^{T} x^{(i)}}}{\\sum_{l=1}^{k} e^{ \\theta_{l}^{T} x^{(i)}} }\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "By taking derivatives, we can show that the gradient of the softmax layer is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta_{j}} \\mathcal{L_D}(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = j\\}  - p(y^{(i)} = j \\mid x^{(i)}, \\theta) \\right) \\right]}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweet activation functions\n",
    "\n",
    "As discussed in the course, the interest of stacking layers is that there is an _activation function_, which allows non-linear interactions between the dimensions (and avoids to only compute a single huge affine transform). Although the `sigmoid` function has been historically the most used, there has been some large developments since. Notably the `ReLU` (Rectified Linear Unit) is one of the major difference in modern networks (we will see more about that in a later course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing the Sigmoid activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def dsigmoid(a):\n",
    "    return a * (1.0 - a)\n",
    "# Function for computing the ReLU activation\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def drelu(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# Function for computing the Tanh activation\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "def dtanh(x): \n",
    "    return np.cosh(x) ^ -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot some simple examples of what these activation functions look like. You can try to rely on these functions in your previous training code and witness the differences in training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_plot import prep_plots, finalize_plots\n",
    "funcs = [('sigmoid',sigmoid,'red'), ('tanh',tanh,'orange'), ('relu',relu,'yellow')]\n",
    "axes = prep_plots([funcs[0][0], funcs[1][0], funcs[2][0]], fig_size=(18,5), fig_num=1)\n",
    "x_plot = np.linspace(-5,5,100)\n",
    "np.maximum(0, x_plot)\n",
    "for f in range(3):\n",
    "    axes[f].plot(x_plot, funcs[f][1](x_plot), color=funcs[f][2], linewidth=4, label=funcs[f][0])\n",
    "finalize_plots(axes, fig_title=\"Activation functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the whole network from scratch\n",
    "\n",
    "You should now have all the tools necessary to apply neural networks from scratch to a more complex problem. In the following exercise, we simply removed any guideline code, and you need to code all the procedure for training a NN and **apply it to audio data**. You will use the spectral features discussed in the previous exercise as an input.\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "  1. Based on the previous neural network, upgrade the code to a 3-layer neural network\n",
    "  2. Implement the *softmax regression* on top of your 3-layer network\n",
    "  3. Use the provided code to perform classification on a pre-defined set of features\n",
    "  4. As previously, change the set of features to assess their different accuracies\n",
    "  5. Evaluate the neural network accuracy for all features combinations\n",
    "  6. What happens if the learning rate is too large ? What is this phenomenon ?\n",
    "  7. (Optional) Perform a more advanced visualization of the learning process.\n",
    "  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "# 0.2 - Pre-process the audio to obtain spectral transforms \n",
    "# (may take around a minute)\n",
    "from helper_data import compute_transforms\n",
    "data_struct = compute_transforms(data_struct)\n",
    "#%%\n",
    "# 0.3 - Compute a set of temporal and spectral features\n",
    "# (may take around 1-2 minutes)\n",
    "from helper_data import compute_features\n",
    "data_struct = compute_features(data_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(y: np.ndarray, axis=0):\n",
    "    return np.exp(y) / (np.sum(np.exp(y), axis=axis))\n",
    "def softmax_d(y: np.ndarray, axis=0):\n",
    "    s = np.sum(np.exp(y), axis=axis)\n",
    "    return np.exp(y) * (s - 1) / s**2\n",
    "\n",
    "def loss(ys_out: np.ndarray, ys_truth: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        ys_out (np.ndarray): a [softmax(), ...] vector,\n",
    "        ys_truth (np.ndarray): [[0, 0, .. 1, 0], ...] vector\n",
    "    Returns:\n",
    "        np.ndarray: loss vector \n",
    "    \"\"\"\n",
    "    # ys_truth is a [[0, 0, .. 1, 0], ...] vector\n",
    "    return -1/ys_out.shape[1] * np.sum(np.dot(ys_truth, np.log(softmax(ys_out, axis=1))))\n",
    "def loss_dw(ys_out: np.ndarray, ys_truth: np.ndarray, xs: np.ndarray) -> np.ndarray:\n",
    "    axis = 1\n",
    "    m = ys_out.shape[axis]\n",
    "    return - 1/m * np.dot(xs, ys_truth - ys_out, axis=axis)\n",
    "\n",
    "fc_layer = lambda w, b:  lambda xs : np.dot(w, xs) + b\n",
    "sigma = sigmoid\n",
    "sigma_d = lambda xs: dsigmoid(sigmoid(xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"spectral_centroid\", \"spectral_bandwidth\", \"spectral_contrast\", \"spectral_flatness\", \"spectral_rolloff\"]\n",
    "moment_names = [\"mean\", \"std\"]\n",
    "\n",
    "feature = \"spectral_centroid\"\n",
    "\n",
    "xs = data_struct[feature][::10]\n",
    "ys = data_struct[\"classes\"][::10]\n",
    "print(xs[0].shape)\n",
    "\n",
    "nb_classes = len(data_struct[\"class_names\"])\n",
    "nb_features = xs[0].shape[1]\n",
    "layer_sizes = [nb_features, 2, nb_classes]\n",
    "layers = [fc_layer, fc_layer]\n",
    "activations = [sigma, softmax]\n",
    "activations_d = [sigma_d, softmax_d]\n",
    "\n",
    "# Init\n",
    "ws = [None] * len(layers)\n",
    "grad_err_ws = [None] * len(layers)\n",
    "for i in range(len(layers)):\n",
    "    ws[i] = np.random.randn(layer_sizes[i+1]+1, layer_sizes[i]+1)\n",
    "    grad_err_ws[i] = np.zeros_like(ws[i])\n",
    "os = [None] * len(layers)\n",
    "deltas = [None] * len(layers)\n",
    "\n",
    "hs = [None] * len(layers)\n",
    "ys_in = [None] * len(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights1 = (np.random.randn(nHiddens, nInputs) - 0.5)      # 1st layer weights\n",
    "# weights2 = (np.random.randn(nOutputs, nHiddens) - 0.5)     # 2nd layer weights\n",
    "# bias1 = (np.random.randn(nHiddens, 1) - 0.5)               # 1st layer biases\n",
    "# bias2 = (np.random.randn(nOutputs, 1) - 0.5)               # 2nd layer biases\n",
    "# Iterate for a fixed number of iterations\n",
    "ys_truth = np.zeros((nb_features, ys.shape[0]))\n",
    "print(ys)\n",
    "print(ys_truth)\n",
    "ys_truth[:, ys] = 1\n",
    "\n",
    "for epoch in range(200):\n",
    "  \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    # forwards\n",
    "    ys_in[0] = xs\n",
    "    for l in range(len(layers)):\n",
    "        w = ws[l]\n",
    "        x_in = ys_in[l]\n",
    "        #\n",
    "        layer = layers[l](ws[l])\n",
    "        hs[l] = layer(x_in)\n",
    "        \n",
    "        ys_in[l+1] = activations[l](h)\n",
    "\n",
    "    ys_out = ys_in[-1]\n",
    "    err = loss(ys_out, ys_truth) + loss_decay(lambda_w, ws[-1])\n",
    "    tss = np.linalg.norm(err)\n",
    "\n",
    "    # backwards\n",
    "    deltas[-1] = sigma_d(hs[-1]) * (ys_truth - y_ins[-1])\n",
    "    for k in range(len(layers)):\n",
    "        l = len(layers) - k\n",
    "        deltas[l] = sigma_d(hs[l]) * np.sum(ws[l+1] * deltas[l+1], axis=2)\n",
    "        # momentum\n",
    "        grad_err_ws_previous = grad_err_ws[l]\n",
    "        grad_err_ws[l] = np.mean(deltas[l] * ys_in[l-1], axis=1, keepdims=True)\n",
    "        ws[l] = momentum * grad_err_ws_previous - eta *grad_err_ws[l]\n",
    "\n",
    "    delta1 = sigma_d(o1) * np.dot(weights2.transpose(), delta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch to enjoy life\n",
    "\n",
    "Up to now, we have been writing every operations by ourselves (in order to better understand the mathematics behind NN). However, there exists of course some simplifying libraries that provide large simplifications to this question.\n",
    "\n",
    "One of the most powerful and complete library of this sort is `Pytorch`, which has been developed for several years (even prior to the recent boom of deep learning). `Pytorch` provides a large set of pre-coded layers, but also **computational graphs** and **autograd**, which are very powerful paradigms allowing to define complex operators and automatically taking derivatives.\n",
    "\n",
    "### Defining our network\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into layers, some of which have learnable parameters which will be optimized during learning. In `PyTorch`, the `nn` package provides higher-level abstractions over raw computational graphs that are useful for building neural networks. The `nn` package defines a set of `Modules`, which are roughly equivalent to neural network layers. A `Module` receives input `Tensors` and computes output `Tensors`, but may also hold internal state such as `Tensors` containing learnable parameters. The nn package also defines a set of useful loss functions that are commonly used when training neural networks.\n",
    "\n",
    "In the following example, we use the `nn` package to show how easy it is to instantiate our previous three-layers network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Define the input dimensions\n",
    "in_size = 1000\n",
    "# Number of neurons in a layer\n",
    "hidden_size = 100\n",
    "# Output (target) dimension\n",
    "output_size = 10\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_size, hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_size, hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_size, output_size),\n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the network\n",
    "\n",
    "Up to this point we have updated the weights of our models by manually performing the gradient descent algorithm (changing the parameters vectors). Although this is not a huge burden for simple optimization algorithms like stochastic gradient descent, in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp or Adam (that we will see later in this course)\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms, and greatly simplfies the training loop associated with training a neural network.\n",
    "\n",
    "For the sake of presentation we will use random inputs $\\mathbf{x}$ that should be matched with random outputs $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example we optimize the model using the Adam algorithm provided by the `optim` package, based on a `MSE` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 1e-4\n",
    "# Loss function that we will use\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "# Optimizer to fit the weights of the network\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "    # Compute the loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Before the backward pass, zero all of the network gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass: compute gradient of the loss with respect to parameters\n",
    "    loss.backward()\n",
    "    # Calling the step function to update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch to classify audio\n",
    "\n",
    "Now that we know the main components of `Pytorch` to define and optimize networks, your assignement is to define a complete classification problem from audio data, by relying on this toolbox\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "  1. Use `Pytorch` to define a model for audio classification\n",
    "  2. Import the audio features dataset and check that your model produces an output\n",
    "  3. Write the optimization loop (think carefully about the _loss function_\n",
    "  4. As previously, change the set of features to assess their different accuracies\n",
    "  5. Think of how you could use more complex features (time series, audio, STFT) to classify your data\n",
    "  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9ede2aee53f8e8bf5c1b30ec575e71e4a41510f580183624109dab457fd1f43"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('env_info': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
