{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows in PyTorch\n",
    "\n",
    "One of the key in modern generative models is to find ways of optimizing the probability distribution of a given set of data. The recent idea of *Normalizing Flows* [[1](#reference1),[2](#reference)] addresses this problem and be able to rely on richer probability distributions. The main idea is to start from a simple probability distribution and approximate a complex multimodal density by *transforming* the simpler density through a sequence of invertible nonlinear transforms. To fully understand this blazing tool, we will see in this tutorial\n",
    "\n",
    "1. The new [PyTorch distributions](#distribs) module and how to use it\n",
    "2. How transforming a distribution is expressed as a [change of variables](#change) leading to a flow\n",
    "3. How we can [chain multiple transforms](#chaining) leading to the overall framework of normalizing flows\n",
    "4. Understanding the original [planar flow](#planar), its parameters and how to implement it\n",
    "5. Defining [learnable flows](#learning) and performing optimization on a target density\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distribs\"></a>\n",
    "### PyTorch distributions\n",
    "\n",
    "In this tutorial, we are going to rely on the novel [PyTorch distributions module](https://pytorch.org/docs/stable/_modules/torch/distributions/), which is defined in `torch.distributions`. Most notably, we are going to rely both on the `Distribution` and `Transform` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "# Imports for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_plot import hdr_plot_style\n",
    "hdr_plot_style()\n",
    "# Define grids of points (for later plots)\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "z = np.array(np.meshgrid(x, x)).transpose(1, 2, 0)\n",
    "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this toolbox, we can already find some of the major probability distributions that we are used to deal with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = distrib.Normal(loc=0, scale=1)\n",
    "p = distrib.Bernoulli(probs=torch.tensor([0.5]))\n",
    "p = distrib.Beta(concentration1=torch.tensor([0.5]), concentration0=torch.tensor([0.5]))\n",
    "p = distrib.Gamma(concentration=torch.tensor([1.0]), rate=torch.tensor([1.0]))\n",
    "p = distrib.Pareto(alpha=torch.tensor([1.0]), scale=torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting aspect of these `Distribution` objects is that we can both obtain some samples from it through the `sample` (or `sample_n`) function, but we can also obtain the analytical density at any given point through the `log_prob` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on a normal\n",
    "n = distrib.Normal(0, 1)\n",
    "# Obtain some samples\n",
    "samples = n.sample((1000, ))\n",
    "# Evaluate true density at given points\n",
    "density = torch.exp(n.log_prob(torch.Tensor(x))).numpy()\n",
    "# Plot both samples and density\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15, 4))\n",
    "ax1.hist(samples, 50, alpha=0.8);\n",
    "ax1.set_title('Empirical samples', fontsize=18);\n",
    "ax2.plot(x, density); ax2.fill_between(x, density, 0, alpha=0.5)\n",
    "ax2.set_title('True density', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"change\"></a>\n",
    "## Transforming distributions\n",
    "\n",
    "### Change of variables and flow\n",
    "\n",
    "In order to transform a probability distribution, we can perform a *change of variable*. As we are interested in probability distributions, we need to *scale* our transformed density so that the total probability still sums to one. This is directly measured with the determinant of our transform.\n",
    "\n",
    "Let $\\mathbf{z}\\in\\mathcal{R}^d$ be a random variable with distribution $q(\\mathbf{z})$ and $f:\\mathcal{R}^d\\rightarrow\\mathcal{R}^d$ an invertible smooth mapping (meaning that $f^{-1} = g$ and $g\\circ f(\\mathbf{z})=\\mathbf{z}'$. We can use $f$ to transform $\\mathbf{z}\\sim q(\\mathbf{z})$. The resulting random variable $\\mathbf{z}'=f(\\mathbf{z})$ has the following probability distribution\n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}')=q(\\mathbf{z})\\left|\\text{ det}\\frac{\\delta f^{-1}}{\\delta \\mathbf{z}'}\\right| = q(\\mathbf{z})\\left|\\text{ det}\\frac{\\delta f}{\\delta \\mathbf{z}}\\right|^{-1}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "where the last equality is obtained through both the inverse function theorem [1] and the property of Jacobians of invertible functions. Therefore, we can transform probability distributions with this property.\n",
    "\n",
    "Fortunately, this can be easily implemented in PyTorch with the `Transform` classes, that already defines some basic probability distribution transforms. For instance, if we define $\\mathbf{z}\\sim q_0(\\mathbf{z})=\\mathcal{N}(0, 1)$, we can apply the transform $\\mathbf{z}'=exp(\\mathbf{z})$ so that $\\mathbf{z}'\\sim q_1(\\mathbf{z}')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = distrib.Normal(0, 1)\n",
    "exp_t = transform.ExpTransform()\n",
    "q1 = distrib.TransformedDistribution(q0, exp_t)\n",
    "samples_q0 = q0.sample((int(1e4),))\n",
    "samples_q1 = q1.sample((int(1e4),))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "ax1.hist(samples_q0, 50, alpha=0.8);\n",
    "ax1.set_title('$q_0 = \\mathcal{N}(0,1)$', fontsize=18);\n",
    "ax2.hist(samples_q1, 50, alpha=0.8, color='g');\n",
    "ax2.set_title('$q_1=exp(q_0)$', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember as the objects `q0` and `q1` are defined as `Distribution`, we can actually observe their true densities instead of just empirical samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_plot_style()\n",
    "x2 = np.linspace(-0.5, 7.5, 1000)\n",
    "q0_density = torch.exp(q0.log_prob(torch.Tensor(x))).numpy()\n",
    "q1_density = torch.exp(q1.log_prob(torch.Tensor(x2))).numpy()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15, 5))\n",
    "ax1.plot(x, q0_density); ax1.fill_between(x, q0_density, 0, alpha=0.5)\n",
    "ax1.set_title('$q_0 = \\mathcal{N}(0,1)$', fontsize=18);\n",
    "ax2.plot(x2, q1_density, color='g'); ax2.fill_between(x2, q1_density, 0, alpha=0.5, color='g')\n",
    "ax2.set_title('$q_1=exp(q_0)$', fontsize=18);\n",
    "fig.savefig('transform.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we obtain here with `q1` is actually the `LogNormal` distribution. Interestingly, several distributions in the `torch.distributions` module are already defined based on `TransformedDistribution`. You can convince yourself of that by lurking in the code of the [`torch.distributions.LogNormal`](https://pytorch.org/docs/stable/_modules/torch/distributions/log_normal.html#LogNormal)\n",
    "\n",
    "<a id=\"chaining\"></a>\n",
    "### Chaining transforms (normalizing flows)\n",
    "\n",
    "Now, if we start with a random vector $\\mathbf{z}_0$ with distribution $q_0$, we can apply a series of mappings $f_i$, $i \\in 1,\\cdots,k$ with $k\\in\\mathcal{N}^{+}$ and obtain a normalizing flow. Hence, if we apply $k$ normalizing flows, we obtain a chain of change of variables\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_k=f_k\\circ f_{k-1}\\circ...\\circ f_1(\\mathbf{z}_0)\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Therefore the distribution of $\\mathbf{z}_k\\sim q_k(\\mathbf{z}_k)$ will be given by\n",
    "   \n",
    "   $$\n",
    "   \\begin{align}\n",
    "   q_k(\\mathbf{z}_k) &= q_0(f_1^{-1} \\circ f_{2}^{-1} \\circ ... \\circ f_k^{-1}(\\mathbf{z}_k))\\prod_{i=1}^k\\left|\\text{det}\\frac{\\delta f^{-1}_i}{\\delta\\mathbf{z}_{i}}\\right|\\\\\n",
    "   &= q_0(\\mathbf{z_0})\\prod_{i=1}^k\\left|\\text{det}\\frac{\\delta f_i}{\\delta\\mathbf{z}_{i-1}}\\right|^{-1}\n",
    "   \\end{align}\n",
    "   \\tag{3}\n",
    "   $$\n",
    "    \n",
    "where we compute the determinant of the Jacobian of each normalizing flow (as explained in the previous section). This series of transformations can transform a simple probability distribution (e.g. Gaussian) into a complicated multi-modal one. As usual, we will rely on log-probabilities to simplify the computation and obtain \n",
    "\n",
    "$$\n",
    "\\text{log} q_K(\\mathbf{z}_k) = \\text{log} q_0(\\mathbf{z}_0) - \\sum_{i=1}^{k} \\text{log} \\left|\\text{det}\\frac{\\delta f_i}{\\delta\\mathbf{z}_{i-1}}\\right| \n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "\n",
    "To be of practical use, however, we can consider only transformations whose determinants of Jacobians are easy to compute. Of course, we can perform any amount of combined transformations, and it also works with multivariate distributions. Here, this is demonstrated by transforming a `MultivariateNormal` successively with an `ExpTransform` and `AffineTransform`. (Note that the final distribution `q2` is defined as a `TransformedDistribution` directly with a *sequence* of transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = distrib.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "# Define an affine transform\n",
    "f1 = transform.ExpTransform()\n",
    "q1 = distrib.TransformedDistribution(q0, f1)\n",
    "# Define an additional transform\n",
    "f2 = transform.AffineTransform(2, torch.Tensor([0.2, 1.5]))\n",
    "# Here I define on purpose q2 as a sequence of transforms on q0\n",
    "q2 = distrib.TransformedDistribution(q0, [f1, f2])\n",
    "# Plot all these lads\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax1.hexbin(z[:,0], z[:,1], C=torch.exp(q0.log_prob(torch.Tensor(z))), cmap='rainbow')\n",
    "ax1.set_title('$q_0 = \\mathcal{N}(\\mathbf{0},\\mathbb{I})$', fontsize=18);\n",
    "ax2.hexbin(z[:,0], z[:,1], C=torch.exp(q1.log_prob(torch.Tensor(z))), cmap='rainbow')\n",
    "ax2.set_title('$q_1=exp(q_0)$', fontsize=18);\n",
    "ax3.hexbin(z[:,0], z[:,1], C=torch.exp(q2.log_prob(torch.Tensor(z))), cmap='rainbow')\n",
    "ax3.set_title('$q_2=Affine(exp(q_0))$', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"planar\"></a>\n",
    "## Normalizing flows\n",
    "\n",
    "Now, we are interested in normalizing flows as we could define our own flows. And, most importantly, we could optimize the parameters of these flow in order to fit complex and richer probability distributions. We will see how this plays out by trying to implement the *planar flow* proposed in the original paper by Rezende [1].\n",
    "\n",
    "### Planar flow\n",
    "\n",
    " A planar normalizing flow is defined as a function of the form\n",
    "\n",
    "   $$\n",
    "   f(\\mathbf{z})=\\mathbf{z}+\\mathbf{u}h(\\mathbf{w}^T\\mathbf{z}+b)\n",
    "   \\tag{5}\n",
    "   $$\n",
    "\n",
    "   where $\\mathbf{u}\\in\\mathbb{R}^D$ and $\\mathbf{w}\\in\\mathbb{R}^D$ are vectors (called here scale and weight), $b\\in\\mathbb{R}$ is a scalar (bias) and $h$ is an activation function. These transform functions are chosen depending on the fact that\n",
    "1. the determinant of their Jacobian can be computed in linear time\n",
    "2. the transformation is invertible (under usually mild conditions only)\n",
    "   \n",
    "As shown in the paper, for the planar flow, the determinant of the Jacobian can be computed in $O(D)$ time by relying on the matrix determinant lemma\n",
    "\n",
    "$$\n",
    "\\psi(\\mathbf{z})=h'(\\mathbf{w}^T\\mathbf{z}+b)\\mathbf{w}\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left|\\text{det}\\frac{\\delta f}{\\delta\\mathbf{z}}\\right| = \\left|\\text{det}\\left(\\mathbf{I}+\\mathbf{u}\\psi(\\mathbf{z})^{T}\\right)\\right|=\\left|1+\\mathbf{u}^T\\psi(\\mathbf{z})\\right|\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "Therefore, we have all definitions that we need to implement this flow as a `Transform` object. Note that here the non-linear activation function $h$ is selected as a $tanh$. Therefore the derivative $h'$ is $1-tanh(x)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(transform.Transform):\n",
    "\n",
    "    def __init__(self, weight, scale, bias):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.bijective = False\n",
    "        self.weight = weight\n",
    "        self.scale = scale\n",
    "        self.bias = bias\n",
    "\n",
    "    def _call(self, z):\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "        return f_z\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "        return abs_log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can witness the effect of this transform on a given `MultivariateNormal` distribution. You should note here that I am using the density estimation for `q0`, but only display empirical samples from `q1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.Tensor([[3., 0]])\n",
    "u = torch.Tensor([[2, 0]])\n",
    "b = torch.Tensor([0])\n",
    "q0 = distrib.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "flow_0 = PlanarFlow(w, u, b)\n",
    "q1 = distrib.TransformedDistribution(q0, flow_0)\n",
    "q1_samples = q1.sample((int(1e6), ))\n",
    "# Plot this\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.hexbin(z[:,0], z[:,1], C=torch.exp(q0.log_prob(torch.Tensor(z))), cmap='rainbow')\n",
    "ax1.set_title('$q_0 = \\mathcal{N}(\\mathbf{0},\\mathbb{I})$', fontsize=18);\n",
    "ax2.hexbin(q1_samples[:,0], q1_samples[:,1], cmap='rainbow')\n",
    "ax2.set_title('$q_1=planar(q_0)$', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this is that the `PlanarFlow` is not invertible in all regions of the space. However, if we recall the mathematical reasoning of the previous section, we can see how the change of variables plays out if we are able to compute the determinant of the Jacobian of this transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0_density = torch.exp(q0.log_prob(torch.Tensor(z)))\n",
    "# Apply our transform on coordinates\n",
    "f_z = flow_0(torch.Tensor(z))\n",
    "# Obtain our density\n",
    "q1_density = q0_density.squeeze() / np.exp(flow_0.log_abs_det_jacobian(torch.Tensor(z)).squeeze())\n",
    "# Plot this\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 5))\n",
    "ax1.hexbin(z[:,0], z[:,1], C=q0_density.numpy().squeeze(), cmap='rainbow')\n",
    "ax1.set_title('$q_0 = \\mathcal{N}(\\mathbf{0},\\mathbb{I})$', fontsize=18);\n",
    "ax2.hexbin(f_z[:,0], f_z[:,1], C=q1_density.numpy().squeeze(), cmap='rainbow')\n",
    "ax2.set_title('$q_1=planar(q_0)$', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we were able to \"split\" our distribution and transform a unimodal gaussian into a multimodal distribution ! Pretty neat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing parameters effects\n",
    "\n",
    "Here, we provide a little toy example so that you can play around with the parameters of the flow in order to get a better understanding of how it operates. As put forward by Rezende [1], this flow is related to the hyperplane defined by $\\mathbf{w}^{T}\\mathbf{z}+b=0$ and transforms the original density by applying a series of contractions and expansions in the direction perpendicular to this hyperplane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_figure=1\n",
    "plt.figure(figsize=(16, 18))\n",
    "for i in np.arange(5):\n",
    "    # Draw a random hyperplane\n",
    "    w = torch.rand(1, 2) * 5\n",
    "    b = torch.rand(1) * 5\n",
    "    for j in np.arange(5):\n",
    "        # Different effects of scaling factor u on the same hyperplane (row)\n",
    "        u = torch.Tensor([[((j < 3) and (j / 2.0) or 0), ((j > 2) and ((j - 2) / 2.0) or 0)]])\n",
    "        flow_0 = PlanarFlow(w, u, b)\n",
    "        q1 = distrib.TransformedDistribution(q0, flow_0)\n",
    "        q1_samples = q1.sample((int(1e6), ))\n",
    "        plt.subplot(5,5,id_figure)\n",
    "        plt.hexbin(q1_samples[:,0], q1_samples[:,1], cmap='rainbow')\n",
    "        plt.title(\"u=(%.1f,%.1f)\"%(u[0,0],u[0,1]) + \" w=(%d,%d)\"%(w[0,0],w[0,1]) + \", \" + \"b=%d\"%b)\n",
    "        plt.xlim([-3, 3])\n",
    "        plt.ylim([-3, 3])\n",
    "        id_figure += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning\"></a>\n",
    "## Optimizing normalizing flows\n",
    "\n",
    "Now that we have this magnificent tool, we would like to apply this in order to learn richer distributions and perform *inference*. Now, we have to deal with the fact that the `Transform` object is not inherently parametric and cannot yet be optimized similarly to other modules.\n",
    "\n",
    "To do so, we will start by defining our own `Flow` class which can be seen both as a `Transform` and also a `Module`that can be optmized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(transform.Transform, nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        transform.Transform.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "    \n",
    "    # Init all parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.01, 0.01)\n",
    "            \n",
    "    # Hacky hash bypass\n",
    "    def __hash__(self):\n",
    "        return nn.Module.__hash__(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to this little trick, we can use the same `PlanarFlow` class as before, that we put back here just to show that the only change is that it now inherits from the `Flow` class (with the small added bonus that now parameters of this flow are also registered in the `Module` interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.scale = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(1))\n",
    "        self.init_parameters()\n",
    "\n",
    "    def _call(self, z):\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "        return f_z\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "        return abs_log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say that we have a given complex density that we aim to model through normalizing flows, such as the following one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_ring(z):\n",
    "    z1, z2 = torch.chunk(z, chunks=2, dim=1)\n",
    "    norm = torch.sqrt(z1 ** 2 + z2 ** 2)\n",
    "    exp1 = torch.exp(-0.5 * ((z1 - 2) / 0.8) ** 2)\n",
    "    exp2 = torch.exp(-0.5 * ((z1 + 2) / 0.8) ** 2)\n",
    "    u = 0.5 * ((norm - 4) / 0.4) ** 2 - torch.log(exp1 + exp2)\n",
    "    return torch.exp(-u)\n",
    "\n",
    "# Plot it\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "z = np.array(np.meshgrid(x, x)).transpose(1, 2, 0)\n",
    "z = np.reshape(z, [z.shape[0] * z.shape[1], -1])\n",
    "plt.hexbin(z[:,0], z[:,1], C=density_ring(torch.Tensor(z)).numpy().squeeze(), cmap='rainbow')\n",
    "plt.title('Target density', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to approximate such a complicated density, we will need to chain multiple planar flows and optimize their parameters to find a suitable approximation. We can do exactly that like in the following (you can see that we start by a simple normal density and perform 16 successive planar flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main class for normalizing flow\n",
    "class NormalizingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, flow_length, density):\n",
    "        super().__init__()\n",
    "        biject = []\n",
    "        for f in range(flow_length):\n",
    "            biject.append(PlanarFlow(dim))\n",
    "        self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_density = density\n",
    "        self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        self.log_det = []\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.log_det = []\n",
    "        # Applies series of flows\n",
    "        for b in range(len(self.bijectors)):\n",
    "            self.log_det.append(self.bijectors[b].log_abs_det_jacobian(z))\n",
    "            z = self.bijectors[b](z)\n",
    "        return z, self.log_det\n",
    "    \n",
    "# Create normalizing flow\n",
    "flow = NormalizingFlow(dim=2, flow_length=16, density=distrib.MultivariateNormal(torch.zeros(2), torch.eye(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the only missing ingredient is the loss function that is simply defined as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(density, zk, log_jacobians):\n",
    "        \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform optimization as usual by defining an optimizer, the parameters it will act on and eventually a learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(flow.parameters(), lr=2e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we perform the loop by sampling a batch (here of 512) from the reference Normal distribution, and then evaluating our loss with respect to the density we want to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_distrib = distrib.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "id_figure=2\n",
    "plt.figure(figsize=(16, 18))\n",
    "plt.subplot(3,4,1)\n",
    "plt.hexbin(z[:,0], z[:,1], C=density_ring(torch.Tensor(z)).numpy().squeeze(), cmap='rainbow')\n",
    "plt.title('Target density', fontsize=15);\n",
    "# Main optimization loop\n",
    "for it in range(10001):\n",
    "    # Draw a sample batch from Normal\n",
    "    samples = ref_distrib.sample((512, ))\n",
    "    # Evaluate flow of transforms\n",
    "    zk, log_jacobians = flow(samples)\n",
    "    # Evaluate loss and backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = loss(density_ring, zk, log_jacobians)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if (it % 1000 == 0):\n",
    "        print('Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        # Draw random samples\n",
    "        samples = ref_distrib.sample((int(1e5), ))\n",
    "        # Evaluate flow and plot\n",
    "        zk, _ = flow(samples)\n",
    "        zk = zk.detach().numpy()\n",
    "        plt.subplot(3,4,id_figure)\n",
    "        plt.hexbin(zk[:,0], zk[:,1], cmap='rainbow')\n",
    "        plt.title('Iter.%i'%(it), fontsize=15);\n",
    "        id_figure += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes this tutorial ! In the next one we will see how to implement more complicated flows and how this can fit in a global inference framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"reference1\"></a>\n",
    "[1] Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\" _arXiv preprint arXiv:1505.05770_ (2015). [link](http://arxiv.org/pdf/1505.05770)\n",
    "\n",
    "[2] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Improving Variational Inference with Inverse Autoregressive Flow.\" _arXiv preprint arXiv:1606.04934_ (2016). [link](https://arxiv.org/abs/1606.04934)\n",
    "\n",
    "[3] Germain, Mathieu, et al. \"Made: masked autoencoder for distribution estimation.\" International Conference on Machine Learning. 2015.\n",
    "\n",
    "### Inspirations and resources\n",
    "\n",
    "https://blog.evjang.com/2018/01/nf1.html  \n",
    "https://github.com/ex4sperans/variational-inference-with-normalizing-flows  \n",
    "https://akosiorek.github.io/ml/2018/04/03/norm_flows.html  \n",
    "https://github.com/abdulfatir/normalizing-flows  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
