{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Machine Learning - Support Vector Machines (SVM)\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)\n",
    "\n",
    "In this course we will cover\n",
    "1. A recap on the [motivation](#motivation) behind Support Vector Machines (SVM)\n",
    "2. A simple use case for [understanding and fitting SVMs](#fitting) with the scikit library\n",
    "3. Developing our own [implementation](#implem) of linear SVMs\n",
    "4. An introuction to [kernels](#improve) in SVMs and how to implement them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "In this tutorial, we will cover a more advanced classification algorithm through the use of *Support Vector Machines* (SVMs). The goal is to gain an intuition of how SVMs works and how to use *Gaussian kernel* with SVMs to find out the decision boundary. The implementation proposed here follows a simplified version of the *Sequential Minimal Optimization* (SMO) algorithm for training support vector machines, which is detailed in the following [paper](http://cs229.stanford.edu/materials/smo.pdf). The complete SMO algorithm can then be quite easily implemented following the [full paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf).\n",
    "\n",
    "Once again, to simplify your work, we provide the following set of functions that you should find in the `data` folder, and in the `helper_plot` file\n",
    "\n",
    "\n",
    "  |**File**|*Explanation*|\n",
    "  |-------:|:------------|\n",
    "  |`svm_linear.mat`|Example data for a quasi-linear problem|\n",
    "  |`svm_gaussian.mat`|Example data for a non-linear but well-defined problem|\n",
    "  |`plot_data`|Allows to plot the set of data points|\n",
    "  |`visualize_boundary`|Plots a non-linear decision boundary from a SVM model|\n",
    "  |`plot_svc_decision_function`|Plots a linear decision boundary|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io as sio\n",
    "from matplotlib import pyplot as plt\n",
    "from helper_plot import plot_data, visualize_boundary, visualize_boundary_linear, hdr_plot_style, plot_svc_decision_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"motivation\"></a>\n",
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous course on neural networks, we learned how to train a simple neuron. We saw that this simple model could be interpreted as defining a line, separating the space into two different regions. This allows to consider problems of *discriminative classification*: rather than modeling each class, we simply find a line that divides the classes from each other. As an example of these _linearly separable_ problems, consider the simple case of a classification task, in which the two classes of points are well separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "hdr_plot_style()\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.65)\n",
    "plt.figure(figsize=(10, 8)); plt.grid(True)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolor='w', cmap='winter');\n",
    "plt.title(\"Linear classification example\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen previously, a single neuron classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification, by optimizing the parameters $\\theta=\\{\\mathbf{w}, b\\}$ of\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y=\\sigma\\left(\\sum_{i = 1}^{n}w_{i}.x_{i} + b\\right)\n",
    "\\label{eq1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "For two dimensional data like that shown here, this is a task we could do by hand. But the major problem that we can directly see, is that there is an infinite number of lines that can perfectly discriminate between the two classes, as exemplified in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.figure(figsize=(10, 8)); plt.grid(True)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolor='w', cmap='winter')\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-w')\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these solutions are all valid, and will mostly depend on the (random) initialization of our parameters. Although these all perfectly discriminate between our samples, we can clearly see that they are more or less \"optimal\" to the problem at hand. So our question today is how could we formalize this problem, and if we could find a formulation to adress and solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notion of _margin_\n",
    "\n",
    "The intuition behind comparing different solutions is that rather than estimating only the line between the classes, we can also look at the *margin* of some width defined around those line. In our simple case, it would look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.figure(figsize=(10, 8)); plt.grid(True)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolor='w', cmap='winter')\n",
    "for m, b, d in [(1, 0.65, 0.25), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.15)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-w')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='w', color='#AAAAAA', alpha=0.6)\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *Support Vector Machines (SVMs)*, the goal is to find that optimal model, in which our separating line maximizes this margin. Hence, SVMs are part of a class of models called *maximum margin* estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fitting\"></a>\n",
    "## Fitting a SVM\n",
    "\n",
    "More formally the goal of SVMs is that given a set of input vectors $ \\mathbf{x} $ separated in two classes (noted $ \\mathbf{x}^{+} $ and $ \\mathbf{x}^{-} $), we would like to find the *optimal boundary* (separating line) between the two classes. To do so, we want to find the separation that gives the *maximal margin* between the two sets. Hence, we can first define the following problem\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\mathbf{w}\\cdot\\mathbf{x}^{+}+\\mathbf{b}\\geq1\\\\\n",
    "\\mathbf{w}\\cdot\\mathbf{x}^{-}+\\mathbf{b}\\leq-1\n",
    "\\end{cases}\n",
    "\\label{eq1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "By setting the variable $ y_{i}={+}/{-}1 $, we can rewrite this problem as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_{i}\\left(x_{i}\\cdot\\mathbf{w}+\\mathbf{b}\\right)-1\\geq0\n",
    "\\label{eq2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The idea is that for some of the input vectors (called *support vectors*), we will have an equality in equation (1), where $ \\mathbf{w}\\cdot\\mathbf{x}^{+}+\\mathbf{b}=1 $ and $ \\mathbf{w}\\cdot\\mathbf{x}^{-}+\\mathbf{b}=-1 $. By substracting those two equation, we see that we are trying to find a solution to \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\mathbf{w}}{\\left\\Vert \\mathbf{w}\\right\\Vert }\\cdot\\left(x^{+}-x^{-}\\right)=\\frac{2}{\\left\\Vert \\mathbf{w}\\right\\Vert }=Width\n",
    "\\label{eq3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Therefore, solving the set of equations amounts to find the maximal margin between the support vectors (as they *support* classification). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the SVM with scikit-learn\n",
    "\n",
    "First, we can see the result of fitting this approach to our original data. To do so, we will use the SVM model found in `scikit-learn` to train an SVM model on this data. To do so, we can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, we will use our quick convenience function that plots the SVM decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8)); plt.grid(True)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='w', cmap='winter')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This separating line is the one that maximizes the margin between our two sets of points.\n",
    "Notice that some training points are right on the margin. These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name. In `scikit-learn`, we can directly obtain the values of these points, stored in the ``support_vectors_`` attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major advantage of SVM is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit. Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin. We can see this, for example, if we plot the model learned from the first 50 or 150 points of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting SVM trained on subsampled dataset\n",
    "def plot_svm_subsample(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor = 'w', cmap='winter')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 150]):\n",
    "    plot_svm_subsample(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, we see the model and the support vectors for 60 training points.\n",
    "In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel.\n",
    "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"implem\"></a>\n",
    "## Implementing our own linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to try to implement our own linear SVM. We recall that, given a set of input vectors $ \\mathbf{x} $ separated in two classes ($ \\mathbf{x}^{+} $ and $ \\mathbf{x}^{-} $, we would like to find the *optimal boundary* (separating line) between the two classes. This means that we are trying to find a solution to \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\mathbf{w}}{\\left\\Vert \\mathbf{w}\\right\\Vert }\\cdot\\left(x^{+}-x^{-}\\right)=\\frac{2}{\\left\\Vert \\mathbf{w}\\right\\Vert }=Width\n",
    "\\label{eq3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Therefore, solving the set of equations amounts to find the maximal margin between the support vectors (as they *support* classification). As seen in the slides, we know that this problem can also be expressed as  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f\\left(x\\right)=\\sum_{i=1}^{m}\\alpha_{i}y_{i}\\left\\langle \\phi\\left(x_{i}\\right),\\phi\\left(x\\right)\\right\\rangle +b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where the function $\\phi(.)$ will be used later to introduce _kernels_, but for now, we will consider that $\\phi(x)=x$, which reduces our problem to\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f\\left(x\\right)=\\sum_{i=1}^{m}\\alpha_{i}y_{i}\\left\\langle x_{i},x \\right\\rangle +b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "we can introduce a kernel $K\\left(x_{i},x\\right)=\\phi\\left(x_{i}\\right)\\cdot\\phi\\left(x\\right)$ in the equation in order to solve non-linear optimization problems. We can use this formulation to perform the *predictions* of the algorithm and in the first part of this tutorial, we will only consider the *linear* kernel $K\\left(x_{i},x\\right)=x_{i}\\cdot x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the way the SVM model that we trained is implemented in `scikit-learn`, we have the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment, our variables of interest are\n",
    "- `support_vectors_` : _Support vectors_ (dataset points that support classification)\n",
    "- `dual_coef_` : The $\\alpha_{i} * y_{i}$ (coefficients from the dual formulation) for the _support_vectors_\n",
    "- `n_support_` : Number of support vectors for each class (ordered)\n",
    "- `_intercept_` : Value of $b$ in our formulation (bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to begin our own implementation of the SVM, we are going to start by computing the _decision function_ of the model. We are going to slightly change the model structure, to better reflect the mathematical formulation than `scikit-learn`. Hence, our model structure and all of its corresponding parameters will be defined as\n",
    "\n",
    "``` Python\n",
    "model = {};\n",
    "model[\"x\"];            # Values of _support vectors_ in input data\n",
    "model[\"y\"];            # Classes labels of _support vectors_\n",
    "model[\"n\"];            # Number of support for each class\n",
    "model[\"b\"];            # Value of bias\n",
    "model[\"alphas\"];       # Value of alphas\n",
    "model[\"kernel\"];       # Type of kernel\n",
    "model[\"w\"];            # Weights of vectors in primal formulation\n",
    "```  \n",
    "\n",
    "In the following, we show how to shift from the `scikit-learn` version to ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Creating a fake (random) model\n",
    "our_model = {};\n",
    "# Values of _support vectors_ in input data\n",
    "our_model[\"x\"] = model.support_vectors_\n",
    "# Corresponding classes of the support vectors\n",
    "our_model[\"y\"] = np.concatenate([[0] * model.n_support_[0], [1] * model.n_support_[1]])\n",
    "# Number of vectors for each class\n",
    "our_model[\"n\"] = model.n_support_\n",
    "# Value of threshold\n",
    "our_model[\"b\"] = model._intercept_            \n",
    "# Values of alphas\n",
    "our_model[\"alphas\"] = model._dual_coef_[0]      \n",
    "print(our_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the _predictions_ of the SVM is performed by calling `model.decision_function(X)` in `scikit_learn`. We are going to implement this behavior ourselves by coding our own `decision_function` function\n",
    "\n",
    "``` Python\n",
    "def decision_function(model, X)\n",
    "    \"\"\"\n",
    "    % Returns a vector of predictions using a trained SVM model. \n",
    "    % model   : svm model returned from svmTrain.\n",
    "    % X       : m x n matrix where each example is a row. \n",
    "    \"\"\"\n",
    "    return pred\n",
    "```\n",
    "\n",
    "For the time being we are using a _linear kernel_, but we might later use a different type of kernel, so we will directly use a `kernel` function which computes our kernel between the support vectors `x_s` and a dataset `X`.\n",
    "\n",
    "***\n",
    "\n",
    "**Exercice**\n",
    "1. Implement the linear kernel function\n",
    "1. Complete the decision function based on the dual formulation\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, X, type='linear'):\n",
    "    \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "\n",
    "    if (type == 'linear'):\n",
    "        k = ...\n",
    "    return k\n",
    "\n",
    "# This function should replicates the scikit-learn one\n",
    "def decision_function(model, X):\n",
    "    \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    \n",
    "    return ...\n",
    "\n",
    "# Evaluate the decision function\n",
    "ax = plt.figure(figsize=(10,8)).gca()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor = 'w', cmap='winter')\n",
    "ax.set_xlim(-1, 4); ax.set_ylim(-1, 6)\n",
    "xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "# Create grid to evaluate model\n",
    "x = np.linspace(xlim[0], xlim[1], 30)\n",
    "yp = np.linspace(ylim[0], ylim[1], 30)\n",
    "Y, Xp = np.meshgrid(yp, x)\n",
    "xy = np.vstack([Xp.ravel(), Y.ravel()]).T\n",
    "P = decision_function(our_model, xy.transpose())\n",
    "P = P.reshape((30, 30))\n",
    "# plot decision boundary and margins\n",
    "ax.contour(Xp, Y, P, colors='w',levels=[-1, 0, 1], alpha=0.9,linestyles=['--', '-', '--']);\n",
    "ax.scatter(our_model[\"x\"][:, 0], our_model[\"x\"][:, 1],s=300, linewidth=2, edgecolor='w', facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning phase\n",
    "\n",
    "In order to perform learning, we will try to maximize the *dual formulation* problem expressed as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "w\\left(\\alpha\\right)=\\sum_{i}\\alpha_{i}-\\frac{1}{2}\\sum_{i}\\sum_{j}y_{i}y_{j}\\alpha_{i}\\alpha_{j}\\left\\langle \\phi\\left(x_{i}\\right),\\phi\\left(x_{j}\\right)\\right\\rangle \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with $0\\leq \\alpha_{i} \\leq C$ and $\\sum_{i=1}^{m}{\\alpha_{i} y_{i}} = 0$. $C$ is a chosen *regularization hyper-parameter*. To ease your work, we will implement only a simplified version of the SMO algorithm which is mainly focused on the optimization aspects. The SMO algorithm selects two $\\alpha$ parameters, $\\alpha_{i}$ and $\\alpha_{j}$ and optimizes the objective value jointly for both these $\\alpha_{i,j}$. Finally it adjusts the $b$ parameter based on the new $\\alpha_{i,j}$. This process is repeated until the $\\alpha_{i,j}$ converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting $\\alpha$\n",
    "\n",
    "The full SMO algorithm use heuristics to choose which $\\alpha_{i}$ and $\\alpha_{j}$ to optimize (which greatly increase the convergence speed. However, for this simplified version, we will simply iterate on all $\\alpha_{i},i=1,\\cdots ,m$ and select a random $\\alpha_{j}$, to perform the joint optimization of $\\alpha_{i}$ and $\\alpha_{j}$\n",
    "\n",
    "### Optimizing $\\alpha_{i}$ and $\\alpha_{j}$\n",
    "\n",
    "To jointly optimize the value, we first find bounds $L$ and $H$ such that $L\\leq \\alpha_{j} \\leq H$ holds to satisfy the constraint of the original problem $0\\leq \\alpha_{i} \\leq C$\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "if\\mbox{ }y_{i}\\neq y_{j}, & L=max\\left(0,\\alpha_{j}-\\alpha_{i}\\right), & H=min\\left(C,C+\\alpha_{j}-\\alpha_{i}\\right)\\\\\n",
    "if\\mbox{ }y_{i}=y_{j}, & L=max\\left(0,\\alpha_{i}+\\alpha_{j}-C\\right), & H=min\\left(C,\\alpha_{i}+\\alpha_{j}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now we want to find $\\alpha_{j}$ so as to maximize the objective function. It can be shown (as in the original paper) that the optimal $\\alpha_{j}$ is given by\n",
    "\n",
    "$$\n",
    "\\alpha_{j}=\\alpha_{j}-\\frac{y_{j}\\left(E_{i}-E_{j}\\right)}{\\eta}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "E_{k} & = & f\\left(x_{k}\\right)-y_{k}\\\\\n",
    "\\eta & = & 2\\left\\langle x_{i},x_{j}\\right\\rangle -\\left\\langle x_{i},x_{i}\\right\\rangle -\\left\\langle x_{j},x_{j}\\right\\rangle \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$E_{k}$ defines the error between the prediction of the SVM on the example $k$ and its desired label. When computing $\\eta$, we can also replace the inner product by a kernel function $K$. Finally, we clip the value of $\\alpha_{j}$ to lie within the selected bounds.\n",
    "\n",
    "$$\n",
    "\\alpha_{j}=\\begin{cases}\n",
    "H & if\\mbox{ }\\alpha_{j}>H\\\\\n",
    "\\alpha_{j} & if\\mbox{ }L\\leq\\alpha_{j}\\leq H\\\\\n",
    "L & if\\mbox{ }\\alpha_{j}<L\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now that we have solved $\\alpha_{j}$, we can update the value for $\\alpha_{i}$ through\n",
    "\n",
    "$$\n",
    "\\alpha_{i}^{t}=\\alpha_{i}^{t-1}+y_{i}y_{j}\\left(\\alpha_{j}^{(t-1)}-\\alpha_{j}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing threshold $b$\n",
    "\n",
    "Finally, we can compute the threshold $b$ to satisfy the original constraints by selecting\n",
    "\n",
    "$$\n",
    "b=\\begin{cases}\n",
    "b_{1} & if\\mbox{ }0<\\alpha_{i}<C\\\\\n",
    "b_{2} & if\\mbox{ }0<\\alpha_{j}<C\\\\\n",
    "\\left(b_{1}+b_{2}\\right)/2 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "b_{1}=b-E_{i}-y_{i}\\left(\\alpha_{i}^{(t-1)}-\\alpha_{i}\\right)\\left\\langle x_{i},x_{i}\\right\\rangle -y_{j}\\left(\\alpha_{j}^{(t-1)}-\\alpha_{j}\\right)\\left\\langle x_{i},x_{j}\\right\\rangle   \n",
    "$$\n",
    "$$\n",
    "b_{2}=b-E_{i}-y_{i}\\left(\\alpha_{i}^{(t-1)}-\\alpha_{i}\\right)\\left\\langle x_{i},x_{j}\\right\\rangle -y_{j}\\left(\\alpha_{j}^{(t-1)}-\\alpha_{j}\\right)\\left\\langle x_{j},x_{j}\\right\\rangle \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of this tutorial, we will compute the main iterations of the algorithm (minimization of the objective function), while relying on a *linear* kernel. This implies that we will only be able to perform linear discrimination. However, remember that the formulation of the SVMs provide an *optimal* and (gloriously) *convex* answer to this problem.\n",
    "\n",
    "```Python\n",
    "def svm_train(X, Y, C, kernelFunction, tol, maxIter):\n",
    "    \"\"\"\n",
    "    % Trains an SVM classifier using a simplified SMO algorithm. \n",
    "    % X       : m x n matrix of m training examples (with n-dimensional features).\n",
    "    % Y       : column vector of class identifiers\n",
    "    % C       : standard SVM regularization parameter\n",
    "    % tol     : tolerance value used for determining equality of floating point numbers. \n",
    "    % maxIter : number of iterations over the dataset before the algorithm stops.\n",
    "    \"\"\"\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Exercise**  \n",
    "<div markdown=\"1\">  \n",
    "\n",
    "  1. Update the `svm_train` code to perform the training of a SVM with linear kernel.\n",
    "  2. Run your `svm_train` function on our linear dataset to obtain similar results as `scikit-learn`\n",
    "  3. Update the framework to display the decision boundary at each iteration of the algorithm.\n",
    "  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train(X, Y_b, C, kernel_type, tol=1e-3, max_iter=5):\n",
    "    \"\"\"\n",
    "    SVMTRAIN Trains an SVM classifier using a simplified version of the SMO \n",
    "    # X       : m x n matrix of m training examples (with n-dimensional features).\n",
    "    # Y       : column vector of class identifiers (0 or 1)\n",
    "    # C       : standard SVM regularization parameter\n",
    "    # tol     : tolerance value used for determining equality of floating point numbers. \n",
    "    # maxIter : number of iterations over the dataset before the algorithm stops.\n",
    "    \"\"\"\n",
    "    # Data parameters\n",
    "    m, n = X.shape\n",
    "    # Variables\n",
    "    alphas = np.zeros((m, 1))\n",
    "    b, passes, eta, L, H = 0, 0, 0, 0, 0\n",
    "    E = np.zeros((m, 1))\n",
    "    Y = Y_b.copy()\n",
    "    Y[Y == 0] = -1\n",
    "    \n",
    "    # Pre-compute the kernel matrix (in practice, the optimized packages will _not_ do this)\n",
    "    K = np.array(kernel(X, X.transpose(), kernel_type))\n",
    "    \n",
    "    # Train\n",
    "    print('Training ...')\n",
    "    while passes < max_iter:\n",
    "        # Check that some alphas changed\n",
    "        num_changed_alphas = 0;\n",
    "        # Iterative over all alpha_i\n",
    "        for i in range(m):\n",
    "            \n",
    "            ######################\n",
    "            # YOUR CODE GOES HERE\n",
    "            # => Perform one pass of the SMO algorithm for each alpha_i\n",
    "            ######################\n",
    "            \n",
    "        if (num_changed_alphas == 0):\n",
    "            passes = passes + 1\n",
    "        else: \n",
    "            passes = 0\n",
    "    print(' Done !');\n",
    "    # Save the model\n",
    "    idx = (alphas > 0).transpose()[0];\n",
    "    model = {} \n",
    "    model[\"x\"] = X[idx, :]\n",
    "    model[\"y\"] = Y[idx]\n",
    "    model[\"b\"] = b\n",
    "    model[\"alphas\"] = alphas[idx, 0] * Y[idx]\n",
    "    model[\"kernel\"] = kernel\n",
    "    return model\n",
    "\n",
    "# Training Linear SVM \n",
    "print('Training Linear SVM ...')\n",
    "# Parameters of the training\n",
    "C = 1                  # C regularization value\n",
    "tolerance = 1e-5       # Tolerance value for thresholds\n",
    "nb_iterations = 50      # Number of iterations\n",
    "# Perform the training (try to fiddle the previous values to see effect).\n",
    "model = svm_train(X, y, C, 'linear', tolerance, nb_iterations);\n",
    "# Evaluate the decision function\n",
    "ax = plt.figure(figsize=(10, 8)).gca()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor = 'w', cmap='winter')\n",
    "ax.set_xlim(-1, 4); ax.set_ylim(-1, 6)\n",
    "xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "# Create grid to evaluate model\n",
    "x = np.linspace(xlim[0], xlim[1], 30)\n",
    "yp = np.linspace(ylim[0], ylim[1], 30)\n",
    "Y, Xp = np.meshgrid(yp, x)\n",
    "xy = np.vstack([Xp.ravel(), Y.ravel()]).T\n",
    "P = decision_function(model, xy.transpose())\n",
    "P = P.reshape((30, 30))\n",
    "# plot decision boundary and margins\n",
    "ax.contour(Xp, Y, P, colors='w',levels=[-1, 0, 1], alpha=0.9,linestyles=['--', '-', '--'])\n",
    "ax.scatter(model[\"x\"][:, 0], model[\"x\"][:, 1],s=300, linewidth=2, edgecolor='w', facecolors='none');\n",
    "plt.title('Our trained SVM :)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond linear problems with _kernels_\n",
    "\n",
    "For the time being, our SVM is only able to solve _linear_ problems. However, the SVM approach becomes truly powerful is when it is combined with *kernels*. As seen in the slides, the major idea is to project our (non-linearly separable) data into a higher-dimensional space, where it becomes linearly separable (allowing to fit for nonlinear relationships with a linear classifier).\n",
    "\n",
    "To better understand this idea, we can look at the same non linearly separable data that we discussed in the slides. Here, we try to fit a _linear_ SVM to this non-linear problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "plt.figure(figsize=(10, 8)); plt.grid(True)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='w', cmap='winter')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that no linear discrimination will *ever* be able to separate this data. As discussed, we need to think about how we might project the data into a higher dimension such that a linear separator *would* be sufficient. For example, one simple projection we could use would be to compute a *radial basis function* centered on the middle group of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this extra data dimension using a three-dimensional plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    plt.figure(figsize=(10, 8));\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, edgecolor='w', cmap='winter')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "plot_3D(elev=30, azim=-50, X=X, y=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane. Here we had to choose and carefully tune our projection. In general, the need to make such a choice is a problem, as we would like to somehow automatically find the best basis functions to use. One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm find the result. This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between **each pair of points**.\n",
    "\n",
    "To see this idea in action first, we will (as previously) use `scikit-learn`, where we can apply _kernelized SVM_ simply by changing our _linear kernel_ to an _RBF (radial basis function)_ kernel, using the ``kernel`` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', C=1E6)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fitted, we can see that our SVM is able to find the largest margin between these two sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8)); plt.grid(True)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='w', cmap='winter')\n",
    "plot_svc_decision_function(model)\n",
    "plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the gaussian kernel\n",
    "\n",
    "As we have seen previously, we know that the complete SVM optimization problem can also be expressed as  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f\\left(x\\right)=\\sum_{i=1}^{m}\\alpha_{i}y_{i}\\left\\langle \\phi\\left(x_{i}\\right),\\phi\\left(x\\right)\\right\\rangle +b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where we can introduce the kernel $K\\left(x_{i},x\\right)=\\phi\\left(x_{i}\\right)\\cdot\\phi\\left(x\\right)$ in the equation in order to solve non-linear optimization problems. We can use this formulation to perform the *predictions* of the algorithm. Consequently, the complete *dual formulation* problem expressed as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "w\\left(\\alpha\\right)=\\sum_{i}\\alpha_{i}-\\frac{1}{2}\\sum_{i}\\sum_{j}y_{i}y_{j}\\alpha_{i}\\alpha_{j}\\left\\langle \\phi\\left(x_{i}\\right),\\phi\\left(x_{j}\\right)\\right\\rangle \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with $0\\leq \\alpha_{i} \\leq C$ and $\\sum_{i=1}^{m}{\\alpha_{i} y_{i}} = 0$. The Gaussian kernel (also called *Radial Basis Function (RBF)* kernel) allows to perform the optimization of non-linear classification problems. It is defined as  \n",
    "\n",
    "$$\n",
    "K\\left(x,y\\right)=exp\\left(\\frac{-\\left\\Vert x-y\\right\\Vert ^{2}}{\\left(2\\sigma^{2}\\right)}\\right)\n",
    "$$\n",
    "\n",
    "As seen in the slides, the underlying idea is that instead of trying to solve a non-linear separation problem in a given space, we might first transform the space into a target space in which the problem is linearly separable. Therefore, we transform the space through a kernel (which is directly expressed in the minimization problem) and solve the linear separation problem in the target space. Given the starter code and the previous exercise, you should be able to directly plug this kernel inside the `svmTrain` and `svmPredict` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Exercise**  \n",
    "\n",
    "  1. Update the `kernel` function code to be able to rely on a Gaussian kernel.\n",
    "  2. Check that you can run the `decision_function` code to rely on this Gaussian kernel.\n",
    "  2. Run the `svm_train` function on our new (non-linear) dataset to obtain similar results as `scikit-learn`\n",
    "  3. Once again, display the decision boundary at each iteration of the algorithm.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, X, type='linear'):\n",
    "    \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    \n",
    "    if (type == 'linear'):\n",
    "        k = ...\n",
    "    elif kernel == 'gaussian':\n",
    "        k = ...\n",
    "    return k\n",
    "\n",
    "\n",
    "plt.title('Traning Dataset')\n",
    "# Parameters of the training\n",
    "C = 1                  # C regularization value\n",
    "tolerance = 1e-3       # Tolerance value for thresholds\n",
    "nb_iterations = 20     # Number of iterations\n",
    "sigma = 0.1            # Variance of Gaussian kernel\n",
    "# Train the SVM with a Gaussian (RBF) kernel\n",
    "model = svm_train(X, y, C, 'gaussian', tolerance, nb_iterations) \n",
    "# Evaluate the decision function\n",
    "ax = plt.figure(figsize=(10, 8)).gca()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor = 'w', cmap='winter')\n",
    "ax.set_xlim(-1, 4); ax.set_ylim(-1, 6)\n",
    "xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "# Create grid to evaluate model\n",
    "x = np.linspace(xlim[0], xlim[1], 30)\n",
    "yp = np.linspace(ylim[0], ylim[1], 30)\n",
    "Y, Xp = np.meshgrid(yp, x)\n",
    "xy = np.vstack([Xp.ravel(), Y.ravel()]).T\n",
    "P = decision_function(model, xy.transpose())\n",
    "P = P.reshape((30, 30))\n",
    "# plot decision boundary and margins\n",
    "ax.contour(Xp, Y, P, colors='w',levels=[-1, 0, 1], alpha=0.9,linestyles=['--', '-', '--'])\n",
    "ax.scatter(model[\"x\"][:, 0], model[\"x\"][:, 1],s=300, linewidth=2, edgecolor='w', facecolors='none');\n",
    "plt.title('Our trained SVM :)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary kernels\n",
    "\n",
    "Now that you have mastered the concept of kernels and updated your code to include the Gaussian kernel, you can augment your classifier and optimizaton problem to include several different kernels as those proposed afterwards.\n",
    "\n",
    "#### Polynomial kernel.\n",
    "\n",
    "Intuitively, the polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of these. \n",
    "\n",
    "$$\n",
    "K\\left(x,y\\right)=\\left(x^{T}y+1\\right)^{d}\n",
    "$$\n",
    "\n",
    "#### Sigmoid (tanh) kernel.\n",
    "\n",
    "This kernel takes two parameters: $a$ and $r$. For $a > 0$, we can view $a$ as a scaling parameter of the input data, and $r$ as a shifting parameter that controls the threshold of mapping. For $a < 0$, the dot-product of the input data is not only scaled but reversed.\n",
    "\n",
    "$$\n",
    "K\\left(x,y\\right)=tanh\\left(kx^{T}y+\\Theta\\right)\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "**Exercise**  \n",
    "\n",
    "  1. Update the `kernel` function code to include these supplementary kernels.\n",
    "  2. Test both the `decision_function` and `svm_train` codes for these new kernels.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio application\n",
    "\n",
    "As seen in the previous tutorial, we have considered only a *binary classification* problem (where elements can only belong to one of two classes). This simplifies the work as we simply need to separate the space between two types of points. However, in real-world problems, we usually want to solve *multi-class* problems with any given number of classes. For this tutorial, we will rely on a simple trick which is to consider that a $n$-class problem can simply be restated as $n$ different binary classification problems. The underlying idea is that each class defines a binary classifier which tells us if a given element is part of this class, or belongs to *any of the other* classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Exercise**  \n",
    "\n",
    "  1. Change the training loop to define $n$separate binary problems\n",
    "  2. For each problem, train an SVM to perform classification.\n",
    "  3. Evaluate the global classification accuracy of your system.\n",
    "  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_data import import_dataset, compute_transforms, compute_features\n",
    "class_path = 'data/classification'\n",
    "# 0.1 - Import the classification dataset\n",
    "data_struct = import_dataset(class_path, 'classification')\n",
    "# 0.2 - Pre-process the audio to obtain spectral transforms \n",
    "data_struct = compute_transforms(data_struct)\n",
    "# 0.3 - Compute a set of temporal and spectral features\n",
    "data_struct = compute_features(data_struct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
